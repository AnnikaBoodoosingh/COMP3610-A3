{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af658a65",
   "metadata": {},
   "source": [
    "**Merge Parquets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b1377b",
   "metadata": {},
   "source": [
    "This code was used for merging back parquets which needed to be saved in chunks due to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c85ba84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing part 1...\n",
      "Processing part 2...\n",
      "Processing part 3...\n",
      "Processing part 4...\n",
      "Processing part 5...\n",
      "✅ Incrementally merged and saved without RAM KO.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "# Path where the parts are saved\n",
    "base_path = r\"C:\\Users\\annik\\OneDrive\\Desktop\\BIG DATA A3\\parquet_parts\"\n",
    "\n",
    "# Output file\n",
    "output_path = os.path.join(base_path, \"Grocery_and_Gourmet_Merged_FULL.parquet\")\n",
    "writer = None\n",
    "\n",
    "# Loop through chunks one at a time\n",
    "for i in range(1, 6):\n",
    "    print(f\"Processing part {i}...\")\n",
    "    chunk_path = os.path.join(base_path, f\"Grocery_and_Gourmet_Merged_part{i}.parquet\")\n",
    "    chunk_df = pd.read_parquet(chunk_path)  # One chunk at a time\n",
    "    \n",
    "    # Convert to PyArrow Table\n",
    "    table = pa.Table.from_pandas(chunk_df)\n",
    "    \n",
    "    # Initialize writer once with schema\n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(output_path, table.schema)\n",
    "    \n",
    "    # Append to file\n",
    "    writer.write_table(table)\n",
    "\n",
    "# Done writing!\n",
    "writer.close()\n",
    "print(\"✅ Incrementally merged and saved without RAM KO.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4536ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rating                                 title_x  \\\n",
      "0     5.0                      Excellent!  Yummy!   \n",
      "1     5.0                       Delicious!!! Yum!   \n",
      "2     5.0  Extremely Delicious, but expensive imo   \n",
      "3     5.0                              Delicious!   \n",
      "4     5.0                             Great taste   \n",
      "5     5.0                                  Yummy!   \n",
      "6     5.0          Excellent tea & smells divine!   \n",
      "7     5.0        Like drinking a wedding bouquet!   \n",
      "8     5.0                              Delicious!   \n",
      "9     5.0               Yummy tea with a cool tin   \n",
      "\n",
      "                                                text  images_x        asin  \\\n",
      "0  Excellent!! Yummy!  Great with other foods and...       NaN  B00CM36GAQ   \n",
      "1  Excellent!  The best!  I use it with my beef a...       NaN  B074J5WVYH   \n",
      "2  These are very tasty. They are extremely soft ...       NaN  B079TRNVHX   \n",
      "3                                       My favorite!       NaN  B07194LN2Z   \n",
      "4     Great for making brownies and crinkle cookies.       NaN  B005CD4196   \n",
      "5                              Yummy for your tummy.       NaN  B07R2X568B   \n",
      "6  This is an excellent white tea and the jasmine...       NaN  B07LDF9V2F   \n",
      "7  This is my #1 favorite tea by my #1 favorite t...       NaN  B000OQXXOA   \n",
      "8  I was never a huge fan of Earl Grey tea until ...       NaN  B073DCD5GC   \n",
      "9  The tea is very good and I love the container....       NaN  B009PO1JTS   \n",
      "\n",
      "  parent_asin                       user_id                     timestamp  \\\n",
      "0  B00CM36GAQ  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ 1970-01-01 00:26:27.854482395   \n",
      "1  B0759B7KLH  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ 1970-01-01 00:26:27.854400380   \n",
      "2  B079TRNVHX  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ 1970-01-01 00:26:27.853224527   \n",
      "3  B07194LN2Z  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ 1970-01-01 00:26:21.313319614   \n",
      "4  B005CD4196  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ 1970-01-01 00:26:21.313294965   \n",
      "5  B0BG8M4XW7  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ 1970-01-01 00:26:21.313263026   \n",
      "6  B07VV7T465  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ 1970-01-01 00:25:52.878483550   \n",
      "7  B00ESE0DC4  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ 1970-01-01 00:25:52.878387841   \n",
      "8  B073DDP593  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ 1970-01-01 00:25:52.878262155   \n",
      "9  B009PO1JTS  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ 1970-01-01 00:25:52.878208860   \n",
      "\n",
      "   helpful_vote  verified_purchase  ... images_y videos  \\\n",
      "0             0               True  ...      NaN    NaN   \n",
      "1             0               True  ...      NaN    NaN   \n",
      "2             1               True  ...      NaN    NaN   \n",
      "3             0               True  ...      NaN    NaN   \n",
      "4             7               True  ...      NaN    NaN   \n",
      "5             0               True  ...      NaN    NaN   \n",
      "6             0               True  ...      NaN    NaN   \n",
      "7             0               True  ...      NaN    NaN   \n",
      "8             1               True  ...      NaN    NaN   \n",
      "9             0               True  ...      NaN    NaN   \n",
      "\n",
      "                   store  categories  \\\n",
      "0         Veetee Dine In         NaN   \n",
      "1         BEN'S ORIGINAL         NaN   \n",
      "2                 Tara's         NaN   \n",
      "3            Oscar Mayer         NaN   \n",
      "4             Toll House         NaN   \n",
      "5            Raisin Bran         NaN   \n",
      "6   Taylors of Harrogate         NaN   \n",
      "7          Harney & Sons         NaN   \n",
      "8  Beantown Tea & Spices         NaN   \n",
      "9         Williamson Tea         NaN   \n",
      "\n",
      "                                             details  bought_together  \\\n",
      "0  {\"Package Dimensions\":\"9.49 x 7.68 x 4.76 inch...             None   \n",
      "1  {\"Package Dimensions\":\"15.94 x 7.56 x 5.43 inc...             None   \n",
      "2  {\"Package Dimensions\":\"5.39 x 3.7 x 3.66 inche...             None   \n",
      "3                                                 {}             None   \n",
      "4  {\"Is Discontinued By Manufacturer\":\"No\",\"Produ...             None   \n",
      "5  {\"Is Discontinued By Manufacturer\":\"No\",\"Packa...             None   \n",
      "6  {\"Package Dimensions\":\"11.65 x 6.85 x 5.28 inc...             None   \n",
      "7  {\"Is Discontinued By Manufacturer\":\"No\",\"Produ...             None   \n",
      "8  {\"Is Discontinued By Manufacturer\":\"No\",\"Item ...             None   \n",
      "9  {\"Is Discontinued By Manufacturer\":\"No\",\"Produ...             None   \n",
      "\n",
      "   subtitle  author  review_length  year  \n",
      "0      None    None              9  1970  \n",
      "1      None    None             26  1970  \n",
      "2      None    None             82  1970  \n",
      "3      None    None              2  1970  \n",
      "4      None    None              7  1970  \n",
      "5      None    None              4  1970  \n",
      "6      None    None             11  1970  \n",
      "7      None    None             21  1970  \n",
      "8      None    None             18  1970  \n",
      "9      None    None             19  1970  \n",
      "\n",
      "[10 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Path to your full Parquet file\n",
    "file_path = r\"C:\\Users\\annik\\OneDrive\\Desktop\\BIG DATA A3\\parquet_parts\\Grocery_and_Gourmet_Merged_FULL.parquet\"\n",
    "\n",
    "# Load the entire dataset in PyArrow (without reading everything into memory at once)\n",
    "dataset = pq.ParquetDataset(file_path)\n",
    "\n",
    "# Read the first few rows (get the first 10 rows)\n",
    "table = dataset.read(columns=None).slice(0, 10)  # The slice here reads the first 10 rows\n",
    "df_sample = table.to_pandas()  # Convert to pandas DataFrame\n",
    "\n",
    "# Show the sample data\n",
    "print(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eba44de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing part 1...\n",
      "Processing part 2...\n",
      "Processing part 3...\n",
      "Processing part 4...\n",
      "Processing part 5...\n",
      "Processing part 6...\n",
      "Processing part 7...\n",
      "Processing part 8...\n",
      "Processing part 9...\n",
      "Processing part 10...\n",
      "Processing part 11...\n",
      "Processing part 12...\n",
      "Processing part 13...\n",
      "Processing part 14...\n",
      "Processing part 15...\n",
      "Processing part 16...\n",
      "Processing part 17...\n",
      "Processing part 18...\n",
      "Processing part 19...\n",
      "Processing part 20...\n",
      "✅ All 20 parts merged and saved at:\n",
      "C:\\Users\\annik\\OneDrive\\Desktop\\BIG DATA A3\\Automotive_Merged_FULL.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "# Folder where your parts are saved\n",
    "base_path = r\"C:\\Users\\annik\\OneDrive\\Desktop\\BIG DATA A3\"\n",
    "\n",
    "# Output full merged file\n",
    "output_path = os.path.join(base_path, \"Automotive_Merged_FULL.parquet\")\n",
    "writer = None\n",
    "\n",
    "# Loop through all 20 parts\n",
    "for i in range(1, 21):\n",
    "    print(f\"Processing part {i}...\")\n",
    "    chunk_path = os.path.join(base_path, f\"Automotive_part{i}.parquet\")\n",
    "    chunk_df = pd.read_parquet(chunk_path)\n",
    "\n",
    "    # Convert chunk to PyArrow Table\n",
    "    table = pa.Table.from_pandas(chunk_df)\n",
    "\n",
    "    # Write to Parquet incrementally\n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(output_path, table.schema)\n",
    "    \n",
    "    writer.write_table(table)\n",
    "\n",
    "# Close the writer when done\n",
    "writer.close()\n",
    "print(\"✅ All 20 parts merged and saved at:\\n\" + output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008e4609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Path to your full Parquet file\n",
    "file_path = r\"C:\\Users\\annik\\OneDrive\\Desktop\\BIG DATA A3\\Automotive_Merged_FULL.parquet\"\n",
    "\n",
    "# Load the entire dataset in PyArrow (without reading everything into memory at once)\n",
    "dataset = pq.ParquetDataset(file_path)\n",
    "\n",
    "# Read the first few rows (get the first 10 rows)\n",
    "table = dataset.read(columns=None).slice(0, 10)  # The slice here reads the first 10 rows\n",
    "df_sample = table.to_pandas()  # Convert to pandas DataFrame\n",
    "\n",
    "# Show the sample data\n",
    "print(df_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
