{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef07374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install fastparquet # easier on memory when converting large dataframe to parquet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4543f74",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454c1140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdata_a3_utils import load_compressed_dataset\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Attempting to load the review data\n",
    "review_dataset = load_compressed_dataset(r\"C:\\Users\\zakar\\Downloads\\amazon_reviews_data\\raw_review_Baby_Products.tar.gz\")\n",
    "review_dataset[\"full\"][0]\n",
    "\n",
    "meta_dataset = load_compressed_dataset(r\"C:\\Users\\zakar\\Downloads\\amazon_reviews_data\\raw_meta_Baby_Products.tar.gz\")\n",
    "meta_dataset[\"full\"][0]\n",
    "\n",
    "df_reviews = review_dataset[\"full\"].to_pandas()\n",
    "print(\"Review dataframe created\")\n",
    "df_meta = meta_dataset[\"full\"].to_pandas()\n",
    "print(\"Meta dataframe created\")\n",
    "\n",
    "merged_df = df_reviews.merge(df_meta, how=\"left\", on=\"parent_asin\")\n",
    "print(\"Dataframe successfully merged!\")\n",
    "\n",
    "\n",
    "\n",
    "merged_df = merged_df[merged_df['rating'].between(1, 5)]\n",
    "merged_df = merged_df[merged_df['text'].notna() & (merged_df['text'].str.strip() != \"\")]\n",
    "merged_df['details'] = merged_df['details'].fillna(\"Unknown\")\n",
    "merged_df['store'] = merged_df['store'].fillna(\"Unknown\")\n",
    "\n",
    "print(\"Remaining rows:\", len(merged_df))\n",
    "print(\"Missing brands from details:\", (merged_df['details'] == 'Unknown').sum())\n",
    "print(\"Missing brands from store:\", (merged_df['store'] == 'Unknown').sum())\n",
    "\n",
    "\n",
    "\n",
    "before = len(merged_df)\n",
    "merged_df = merged_df.drop_duplicates(subset=['user_id', 'asin', 'text'], keep='first')\n",
    "after = len(merged_df)\n",
    "print(f\"Removed {before - after} duplicate reviews\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to compute token count in review text\n",
    "def review_length(text):\n",
    "    if isinstance(text, str):\n",
    "        tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "        return len(tokens)\n",
    "    else:\n",
    "        return 0  # or np.nan if you prefer\n",
    "\n",
    "# Apply to create the 'review_length' column\n",
    "merged_df['review_length'] = merged_df['text'].apply(review_length)\n",
    "print(\"Review length column created!\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert the 'timestamp' column to datetime\n",
    "merged_df['timestamp'] = pd.to_datetime(merged_df['timestamp'], errors='coerce')\n",
    "print(\"Converting timestamp column to datetime\")\n",
    "\n",
    "# Extract the year\n",
    "merged_df['year'] = merged_df['timestamp'].dt.year\n",
    "merged_df[['text', 'review_length', 'timestamp', 'year']].head()\n",
    "print(\"Year column extracted and created!\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Handling complex columns... converting values to NaN\")\n",
    "# List of complex columns to handle\n",
    "complex_cols = ['images_x', 'images_y', 'videos', 'features', 'description', 'categories']\n",
    "\n",
    "# Temporarily set complex columns to NaN\n",
    "for col in complex_cols:\n",
    "    merged_df[col] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "print(\"Saving cleaned category...\")\n",
    "#Saving cleaned and merged dataset to parquet\n",
    "merged_df.to_parquet(\"Baby_Products_Merged.parquet\", index=False, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6c9585",
   "metadata": {},
   "source": [
    "Troubleshooting for some complex columns... later decided they were not needed for future tasks. Therefore, converting them to NaN and will drop them after merging the full datatset (some categories were already cleaned and saved with these columns, hence we can't drop them yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cfa5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for col in merged_df.columns:\n",
    "    if merged_df[col].apply(lambda x: isinstance(x, list)).any():\n",
    "        print(f\"Converting {col} from list to string...\")\n",
    "        merged_df[col] = merged_df[col].apply(json.dumps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc7266b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rating', 'title_x', 'text', 'images_x', 'asin', 'parent_asin',\n",
       "       'user_id', 'timestamp', 'helpful_vote', 'verified_purchase',\n",
       "       'main_category', 'title_y', 'average_rating', 'rating_number',\n",
       "       'features', 'description', 'price', 'images_y', 'videos', 'store',\n",
       "       'categories', 'details', 'bought_together', 'subtitle', 'author',\n",
       "       'review_length', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "422f90f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating                      float64\n",
       "title_x                      object\n",
       "text                         object\n",
       "images_x                     object\n",
       "asin                         object\n",
       "parent_asin                  object\n",
       "user_id                      object\n",
       "timestamp            datetime64[ns]\n",
       "helpful_vote                  int64\n",
       "verified_purchase              bool\n",
       "main_category                object\n",
       "title_y                      object\n",
       "average_rating              float64\n",
       "rating_number                 int64\n",
       "features                     object\n",
       "description                  object\n",
       "price                        object\n",
       "images_y                     object\n",
       "videos                       object\n",
       "store                        object\n",
       "categories                   object\n",
       "details                      object\n",
       "bought_together              object\n",
       "subtitle                     object\n",
       "author                       object\n",
       "review_length                 int64\n",
       "year                          int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90fd8361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problematic columns: ['images_y', 'videos']\n"
     ]
    }
   ],
   "source": [
    "problematic_cols = [\n",
    "    col for col in merged_df.columns\n",
    "    if merged_df[col].apply(lambda x: isinstance(x, (list, dict))).any()\n",
    "]\n",
    "\n",
    "print(\"Problematic columns:\", problematic_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6c79a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safely serializing column: images_y\n",
      "Safely serializing column: videos\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def deep_convert(obj):\n",
    "    \"\"\"Recursively convert any ndarray to list for JSON serialization.\"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, list):\n",
    "        return [deep_convert(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: deep_convert(value) for key, value in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def safe_serialize(x):\n",
    "    try:\n",
    "        return json.dumps(deep_convert(x))\n",
    "    except Exception as e:\n",
    "        print(f\"Serialization failed for: {x} with error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply safe serialization to problematic columns\n",
    "for col in problematic_cols:\n",
    "    print(f\"Safely serializing column: {col}\")\n",
    "    merged_df[col] = merged_df[col].apply(safe_serialize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c60c7284",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['images_x'] = merged_df['images_x'].apply(safe_serialize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc77a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['features'] = merged_df['features'].apply(safe_serialize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f329fb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex columns: ['description', 'categories']\n"
     ]
    }
   ],
   "source": [
    "def is_complex(val):\n",
    "    return isinstance(val, (list, dict, np.ndarray))\n",
    "\n",
    "complex_cols = [col for col in merged_df.columns if merged_df[col].apply(is_complex).any()]\n",
    "print(\"Complex columns:\", complex_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1e94ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of complex columns to handle\n",
    "complex_cols = ['images_x', 'images_y', 'videos', 'features', 'description', 'categories']\n",
    "\n",
    "# Temporarily set complex columns to NaN\n",
    "for col in complex_cols:\n",
    "    merged_df[col] = np.nan\n",
    "\n",
    "# Save the DataFrame as parquet\n",
    "merged_df.to_parquet(\"Appliances_Merged.parquet\", index=False, engine='fastparquet')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
